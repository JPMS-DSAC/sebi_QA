{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Processing Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/deepti-\n",
      "[nltk_data]     saravanan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepti-saravanan/.pyenv/versions/3.7.3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading trained model...\n",
      "bert_qa.joblib already downloaded\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.metrics import edit_distance\n",
    "#Definitions dictionary\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "defdict = {}\n",
    "path_to_json = './Definitions/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "for i in json_files:\n",
    "    with open('./Definitions/'+i) as json_file: \n",
    "        data = json.load(json_file)\n",
    "    defdict.update(data)\n",
    "definitions = list(defdict.keys())\n",
    "defvalues = list(defdict.values())\n",
    "\n",
    "docs = [\"Issue and Listing of Non-Convertible Redeemable Preference Shares\", \"Investment Advisers\", \"Depositories and Participants\", \"Mutual Funds\", \"Employees' Service\", \"Substantial Acquisition of Shares and Takeovers\", \"Appointment of Administrator and Procedure for Refunding to the Investors\", \"Prohibition of Fraudulent and Unfair Trade Practices relating to Securities Market\", \"Know Your Client\", \"Prohibition of Insider Trading\", \"Merchant Bankers\", \"Issue and Listing of Securitised Debt Instruments and Security Receipts\", \"Delisting of Equity Shares\", \"Issue of Capital and Disclosure Requirements\", \"Foreign Venture Capital Investor\", \"Procedure for Board Meetings\", \"Custodian\", \"Ombudsman\", \"Investor Protection and Education Fund\", \"Foreign Portfolio Investors\", \"Issue of Sweat Equity\", \"Collective Investment Scheme\", \"Portfolio Managers\", \"Research Analysts\", \"Procedure for Search and Seizure\", \"Issue Of Capital And Disclosure Requirements\", \"Share Based Employee Benefits\", \"Debenture Trustees\", \"Alternative Investment Funds\", \"Stock Exchanges and Clearing Corporations\", \" Self Regulatory Organisations\", \"Settlement Proceedings\", \"Issue and Listing of Municipal Debt Securities\", \"Buy-back of Securities\", \"Issue and Listing of Debt Securities\", \"Infrastructure Investment Trusts\", \" Stock Brokers\", \"Listing Obligations and Disclosure Requirements\", \"Registrars to an Issue and Share Transfer Agents\", \"Real Estate Investment Trusts\", \"Intermediaries\", \"Certification of Associated Persons in the Securities Markets\", \"Credit Rating Agencies\", \"Regulatory Fee on Stock Exchanges\", \"Underwriters\", \"Buy Back Of Securities\", \"Bankers to an Issue\", \"Central Database of Market Participants\"]\n",
    "\n",
    "#Regulations\n",
    "finalclean48 = []\n",
    "with open('cleanedregulations48.pkl','rb') as f:\n",
    "    finalclean48 = pickle.load(f)    \n",
    "docregs = finalclean48\n",
    "\n",
    "#Topics documentwise\n",
    "finalcleantopics48 = []\n",
    "with open('cleanedregtopics48.pkl','rb') as f:\n",
    "    finalcleantopics48 = pickle.load(f)    \n",
    "finaltopics = finalcleantopics48\n",
    "\n",
    "#vocab definitions\n",
    "mainvocab = []\n",
    "with open('mainvocab.pkl','rb') as f:\n",
    "    mainvocab = pickle.load(f) \n",
    "\n",
    "vocabdef = []\n",
    "with open('vocabdef.pkl','rb') as f:\n",
    "    vocabdef = pickle.load(f) \n",
    "\n",
    "#Additional Documents\n",
    "\n",
    "with open('concept_filenames.pkl','rb') as f:\n",
    "    cfile = pickle.load(f) \n",
    "with open('concept_sentences.pkl','rb') as f:\n",
    "    csent = pickle.load(f) \n",
    "with open('concept_text.pkl','rb') as f:\n",
    "    ctext = pickle.load(f) \n",
    "\n",
    "with open('informal_filenames.pkl','rb') as f:\n",
    "    ifile = pickle.load(f) \n",
    "with open('informal_sentences.pkl','rb') as f:\n",
    "    isent = pickle.load(f) \n",
    "with open('informal_text.pkl','rb') as f:\n",
    "    itext = pickle.load(f) \n",
    "    \n",
    "with open('miscvocab.pkl', 'rb') as f:\n",
    "    miscvocab = pickle.load(f)\n",
    "    \n",
    "#Legal Case files\n",
    "with open('case_filenames.pkl','rb') as f:\n",
    "    lfile = pickle.load(f) \n",
    "with open('case_sentences.pkl','rb') as f:\n",
    "    lsent = pickle.load(f) \n",
    "with open('case_text.pkl','rb') as f:\n",
    "    ltext = pickle.load(f) \n",
    "    \n",
    "with open('casevocab.pkl', 'rb') as f:\n",
    "    casevocab = pickle.load(f)\n",
    "\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "#Legal pre-trained model for synonyms\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "#m = KeyedVectors.load_word2vec_format('/home/deepti-saravanan/Music/raw-legal/no_replacements/legalmodeltext.txt',binary=False, unicode_errors='ignore')\n",
    "#modelvocab = list(m.vocab.keys())\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "STOPWORDS = set(\n",
    "    list(stop_words.ENGLISH_STOP_WORDS) +\\\n",
    "    stopwords.words('english') +\\\n",
    "    ['mm', 'section', 'subsection', 'schedule', '-PRON-', 'chapter', 'regulation', 'repealed', 'thereto','unpublishe', 'thereunder','guideline', 'reference','onus','make','Page','Securities','Exchange','India'])\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from cdqa.utils.filters import filter_paragraphs\n",
    "from cdqa.pipeline import QAPipeline\n",
    "\n",
    "from cdqa.utils.download import download_model\n",
    "download_model(model='bert-squad_1.1', dir='./models')\n",
    "\n",
    "with open('glossary.json') as f:\n",
    "    glossary = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryvocab(query):\n",
    "    query1 = \"\"\n",
    "    qvocab=[]\n",
    "    for i in query.split():\n",
    "        if(i not in stopwords.words()):\n",
    "            if(i != 'What' and i != 'what' and i!='How'):\n",
    "                qvocab.append(i)\n",
    "                query1 = query1 + \" \" + i\n",
    "    query = query1\n",
    "    return qvocab, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regextract(docinput, docs, docregs, mainvocab, vocabdef, query, glossary):\n",
    "    \n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    v = []\n",
    "    \n",
    "    if(docinput == 'All'):\n",
    "        rdocs = []\n",
    "        for i in glossary.keys():\n",
    "            if(i in query.lower()):\n",
    "                dval = glossary[i]\n",
    "                for j in dval:\n",
    "                    if(j not in rdocs):\n",
    "                        rdocs.append(j)\n",
    "        if(len(rdocs)>0):\n",
    "            docinput = []\n",
    "            for i in rdocs:\n",
    "                docinput.append(docs[i])\n",
    "        else:\n",
    "            docinput = ['All']\n",
    "            \n",
    "    else:\n",
    "        d = [docinput]\n",
    "        docinput = d\n",
    "    \n",
    "    for dval in docinput:\n",
    "    \n",
    "        if(dval != 'All'):\n",
    "            ind = docs.index(dval)\n",
    "\n",
    "            rules = docregs[ind]\n",
    "        \n",
    "            t=-1\n",
    "            vocab0 = []\n",
    "            for i in rules:\n",
    "                t=t+1\n",
    "                vocab0.append(i.split())\n",
    "            \n",
    "            for i in vocab0:\n",
    "                for j in i:\n",
    "                    if(len(j)>2):\n",
    "                        for m in range(len(symbols)):\n",
    "                            j = np.char.replace(j, symbols[m], '')\n",
    "                            j = np.char.replace(j, \",\", \"\")\n",
    "                            j = np.array2string(j)\n",
    "                            j = j.replace(\"'\",\"\")\n",
    "                        if(j not in v):\n",
    "                            v.append(j)\n",
    "        else:\n",
    "            rules = []\n",
    "            for i in docregs:\n",
    "                for j in i:\n",
    "                    rules.append(j)\n",
    "            vocab0 = mainvocab\n",
    "        \n",
    "            for j in vocab0:\n",
    "                if(len(j)>2):\n",
    "                    for m in range(len(symbols)):\n",
    "                        j = np.char.replace(j, symbols[m], '')\n",
    "                        j = np.char.replace(j, \",\", \"\")\n",
    "                        j = np.array2string(j)\n",
    "                        j = j.replace(\"'\",\"\")\n",
    "                    if(j not in v):\n",
    "                        v.append(j)\n",
    "    \n",
    "    if('All' not in docinput):\n",
    "        vdef = []\n",
    "        for i in v:\n",
    "            if(i in mainvocab and len(i)>2):\n",
    "                vindex = mainvocab.index(i)\n",
    "                vdef.append(vocabdef[vindex])\n",
    "    else:\n",
    "        vdef = vocabdef\n",
    "        \n",
    "    \n",
    "    return rules, v, vdef, docinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querypreprocess(query, qvocab, definitions, finaltopics):\n",
    "    if('regulations' in qvocab):\n",
    "        qvocab.remove('regulations')\n",
    "    if('rules' in qvocab):\n",
    "        qvocab.remove('rules')\n",
    "    if('rule' in qvocab):\n",
    "        qvocab.remove('rule')\n",
    "    if('chapter' in qvocab):\n",
    "        qvocab.remove('chapter')\n",
    "    if('section' in qvocab):\n",
    "        qvocab.remove('section')\n",
    "    if('sub' in qvocab):\n",
    "        qvocab.remove('sub')\n",
    "    if('SEBI' in qvocab):\n",
    "        qvocab.remove('SEBI')\n",
    "    if('means' in qvocab):\n",
    "        qvocab.remove('means')\n",
    "    if('shall' in qvocab):\n",
    "        qvocab.remove('shall')\n",
    "    if('Securities' in qvocab):\n",
    "        qvocab.remove('Securities')\n",
    "    if('Exchange' in qvocab):\n",
    "        qvocab.remove('Exchange')\n",
    "    if('pertaining' in qvocab):\n",
    "        qvocab.remove('pertaining')\n",
    "    if('India' in qvocab):\n",
    "        qvocab.remove('India')\n",
    "        \n",
    "    query = \"\"\n",
    "    for i in qvocab:\n",
    "        query = query + \" \" + i\n",
    "        \n",
    "    synwords = []\n",
    "    importantwords = []\n",
    "    for i in qvocab:\n",
    "        #print(i)\n",
    "        flag = 0\n",
    "        for j in definitions:\n",
    "            if(i in j):\n",
    "                flag = 1\n",
    "                break\n",
    "        if(flag == 1):\n",
    "            importantwords.append(i)\n",
    "        if(flag == 0):\n",
    "            synwords.append(i)\n",
    "    wforms = []\n",
    "    for i in synwords:\n",
    "        wforms.append(wordnet.morphy(i, wordnet.VERB))\n",
    "        wforms.append(wordnet.morphy(i, wordnet.NOUN))\n",
    "    for j in wforms:\n",
    "        if(j != '' and j not in synwords and j != None):\n",
    "            synwords.append(j)\n",
    "    snew = []   \n",
    "    for i in synwords:\n",
    "        for j in finaltopics:\n",
    "            for k in j:\n",
    "                if(i != k):\n",
    "                    snew.append(i)\n",
    "    synwords = snew\n",
    "    \n",
    "    expansionwords = []\n",
    "    for i in qvocab:\n",
    "        flag=0\n",
    "        for j in finaltopics:\n",
    "            for k in j:\n",
    "                if(i == k):\n",
    "                    flag=1\n",
    "        if(flag==1):\n",
    "            if(i not in importantwords):\n",
    "                importantwords.append(i)\n",
    "        if(flag==0):\n",
    "            for j0 in definitions:\n",
    "                if(i in j0):\n",
    "                    expansionwords.append(j0)\n",
    "                    break\n",
    "    \n",
    "    qnew = query\n",
    "    for j in expansionwords:\n",
    "        k=0\n",
    "        for i in definitions:\n",
    "            if(i==j):\n",
    "                s = i\n",
    "                s = s + defvalues[k]\n",
    "                qnew = qnew + ' ' + s\n",
    "                break\n",
    "            k=k+1\n",
    "        else:\n",
    "            qnew = qnew + ' ' + j\n",
    "    query = qnew\n",
    "    \n",
    "    sent = nlp(query)\n",
    "    t=0\n",
    "    for token in sent:\n",
    "        if(str(token) in importantwords):\n",
    "            if(t!=0):\n",
    "                if(str(sent[t-1]) not in importantwords):\n",
    "                    importantwords.append(str(sent[t-1]))\n",
    "            if(token.tag_ == 'VB'):\n",
    "                importantwords.append(str(token))\n",
    "        t+=1\n",
    "    \n",
    "    return query, qvocab, synwords, importantwords, expansionwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulations Retrieval Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legalembedding(synwords, modelvocab, m, vocab):\n",
    "    \n",
    "    synonyms0 = []\n",
    "    \n",
    "    for i in synwords:\n",
    "        if(i in modelvocab):\n",
    "            l = m.most_similar(i)\n",
    "            for j in l:\n",
    "                if(j in vocab):\n",
    "                    synonyms0.append(j)\n",
    "    return synonyms0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def defitionssynonyms(vocab, synwords, vdef, vocabdef, mainvocab):\n",
    "    \n",
    "    #Definitions of all synwords\n",
    "    syndef = []\n",
    "    for i in synwords:\n",
    "        if(i == None):\n",
    "            synwords.remove(i)\n",
    "        else:\n",
    "            syns = wordnet.synsets(i)\n",
    "            if(syns != []):\n",
    "                syndef.append(syns[0].definition())\n",
    "    \n",
    "    #Comparison of definitions\n",
    "    synonyms1 = []\n",
    "    if(syndef != []):\n",
    "        p=0\n",
    "        for i in syndef:\n",
    "            for j in vdef:\n",
    "                vind = vocabdef.index(j)\n",
    "                if(synwords[p] in j):\n",
    "                    if(mainvocab[vind] not in synonyms1):\n",
    "                        synonyms1.append(mainvocab[vind])\n",
    "            p=p+1\n",
    "            \n",
    "    return synonyms1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarityscore(synonyms0, synonyms1,synwords):\n",
    "    synonyms2 = []\n",
    "    for i in synonyms0:\n",
    "        if(i not in synonyms2):\n",
    "            synonyms2.append(i)\n",
    "    for i in synonyms1:\n",
    "        if(i not in synonyms2):\n",
    "            synonyms2.append(i)\n",
    "    \n",
    "    synonyms = []\n",
    "    for i in synwords:\n",
    "        for j in synonyms2:\n",
    "            score = []\n",
    "            for syn in wordnet.synsets(i):\n",
    "                for syn1 in wordnet.synsets(j):\n",
    "                    score.append(syn.wup_similarity(syn1))\n",
    "            if(None not in score):\n",
    "                score.sort(reverse=True)\n",
    "                if(len(score)>0):\n",
    "                    if(score[0] > 0.5):\n",
    "                        synonyms.append(j)\n",
    "                \n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalsynonyms(synwords, modelvocab, m, vocab, query, vdef, vocabdef, mainvocab):\n",
    "    \n",
    "    synonyms0 = legalembedding(synwords, modelvocab, m, vocab)\n",
    "    synonyms1 = defitionssynonyms(vocab, synwords, vdef, vocabdef, mainvocab)\n",
    "    synonyms = similarityscore(synonyms0, synonyms1,synwords)\n",
    "    \n",
    "    while(None in synonyms):\n",
    "        synonyms.remove(None)\n",
    "        \n",
    "    for i in synonyms:\n",
    "        for j in vocab:\n",
    "            if(edit_distance(i,j) < 4):\n",
    "                if(j not in synonyms):\n",
    "                    synonyms.append(j)\n",
    "                \n",
    "    for i in synonyms:\n",
    "        query = query + ' ' + i\n",
    "        \n",
    "    qvocab = []\n",
    "    for i in query.split():\n",
    "        qvocab.append(i)\n",
    "    \n",
    "    return synonyms, query, qvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    "    return np.char.lower(data)\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = STOPWORDS\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidfreg(docregs, docinput, category):\n",
    "    \n",
    "    if(category == 'regulations'):\n",
    "        d = [] \n",
    "    \n",
    "        for i in docinput:\n",
    "            if(i in docs):\n",
    "                ind = docs.index(i)\n",
    "                r0 = docregs[ind]\n",
    "                d.append(r0)\n",
    "            \n",
    "        if(len(d)>0):\n",
    "            docregs = d\n",
    "        #print(docregs)\n",
    "\n",
    "    regstring = []\n",
    "    origregstring = []\n",
    "    for i in docregs:\n",
    "        for j in i:\n",
    "            origregstring.append(j)\n",
    "            data = preprocess(j)\n",
    "            regstring.append(data)        \n",
    "    wordregs = regstring\n",
    "    \n",
    "    t=0\n",
    "    remind = []\n",
    "    for i in regstring:\n",
    "        if(i=='\\xa0\\n' or i == ''):\n",
    "            remind.append(t)\n",
    "            regstring.remove(i)\n",
    "            wordregs.remove(wordregs[t])\n",
    "        t+=1\n",
    "        \n",
    "    origreg = []\n",
    "    t=0\n",
    "    while(t<len(origregstring)):\n",
    "        if(t not in remind):\n",
    "            origreg.append(origregstring[t])\n",
    "        t+=1\n",
    "            \n",
    "    wordregs = regstring\n",
    "    \n",
    "    DF = {}\n",
    "    wlist = []\n",
    "    for i in range(len(wordregs)):\n",
    "        tokens = regstring[i]\n",
    "        for w in tokens.split():\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "                wlist.append(w)\n",
    "                \n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    \n",
    "    total_vocab = [x for x in DF]\n",
    "    total_vocab_size = len(DF)\n",
    "    \n",
    "    tf_idf = {}\n",
    "    N = len(regstring)\n",
    "    for i in range(N):\n",
    "        tokens = regstring[i].split()\n",
    "        counter = Counter(tokens)\n",
    "        for token in np.unique(tokens):\n",
    "            if(token in wlist):\n",
    "                tf = counter[token]/len(tokens)\n",
    "                df = DF[token]\n",
    "                idf = np.log((N+1)/(df+1))\n",
    "                try:\n",
    "                    tf_idf[token] = tf_idf[token] + tf*idf\n",
    "                except:\n",
    "                    tf_idf[token] = tf*idf                \n",
    "            \n",
    "    return DF, N, tf_idf, total_vocab, total_vocab_size, wordregs, regstring, origreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tfidf_regs.pkl','wb') as f:\n",
    "    pickle.dump(tf_idf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_vector(qvocab, N, total_vocab, DF):\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    querytokens = qvocab\n",
    "    counter = Counter(querytokens)\n",
    "    words_count = len(querytokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(querytokens):\n",
    "        if(token in DF):\n",
    "            tf = counter[token]/words_count\n",
    "            df = DF[token]\n",
    "            idf = math.log((N+1)/(df+1))\n",
    "\n",
    "            try:\n",
    "                ind = total_vocab.index(token)\n",
    "                Q[ind] = tf*idf\n",
    "            except:\n",
    "                pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, query, total_vocab, N, qvocab, DF, origreg, D, importantwords):\n",
    "        \n",
    "    querytokensall = qvocab\n",
    "\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = q_vector(qvocab, N, total_vocab, DF)\n",
    "    \n",
    "    t=0\n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        t=t+1\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[::-1]\n",
    "    \n",
    "    answers = []\n",
    "    for i in out:\n",
    "        flag=0\n",
    "        for j in querytokensall:\n",
    "            if(j in origreg[i].split()):\n",
    "                if(j in importantwords):\n",
    "                    flag=flag+2\n",
    "                else:\n",
    "                    flag=flag+1\n",
    "        if(flag>1):\n",
    "            answers.append([flag,origreg[i]])\n",
    "    answers.sort(reverse=True)\n",
    "\n",
    "    \n",
    "    if(k<len(answers)):\n",
    "        return answers[0:k]\n",
    "    else:\n",
    "        return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg,importantwords):\n",
    "    \n",
    "    D = np.zeros((N, len(total_vocab)))\n",
    "    for i in tf_idf:\n",
    "        try:\n",
    "            ind = total_vocab.index(i[1])\n",
    "            D[i[0]][ind] = tf_idf[i]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    A = cosine_similarity(k, query, total_vocab, N, qvocab, DF, origreg, D, importantwords)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answermod(A, queryinput):\n",
    "    \n",
    "    ans = []\n",
    "    ansreg = []\n",
    "    \n",
    "    t=0\n",
    "    while(t<len(A)):\n",
    "        A[t][0] = 'title'\n",
    "        A[t][1] = [A[t][1]]\n",
    "        t+=1\n",
    "    t=0\n",
    "    df = pd.DataFrame(A,columns=['title','paragraphs'])\n",
    "    \n",
    "    cdqa_pipeline = QAPipeline(reader='./models/bert_qa.joblib')\n",
    "    cdqa_pipeline.fit_retriever(df=df)\n",
    "    \n",
    "    prediction = cdqa_pipeline.predict(queryinput)\n",
    "    \n",
    "    return prediction[0], prediction[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QnAmodel(queryinput, docinput,k,category):\n",
    "    print('Preprocessing query......')\n",
    "    qvocab, query = queryvocab(queryinput)\n",
    "    if(category == 'regulations'):\n",
    "        rules, vocab, vdef, docinput = regextract(docinput, docs, docregs, mainvocab, vocabdef, queryinput, glossary)\n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('tf-idf calculation in progress.....')\n",
    "        DF, N, tf_idf, total_vocab, total_vocab_size, wordregs, regstring, origreg = tfidfreg(docregs,docinput,category)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "    elif(category == 'misc'):\n",
    "        \n",
    "        rules = ctext\n",
    "        rules.extend(itext)\n",
    "        \n",
    "        #for lol in docregs:\n",
    "        #    rules.extend(lol)\n",
    "        vocab = miscvocab\n",
    "        #vocab.extend(mainvocab)\n",
    "        N = 447\n",
    "        #N = 6379\n",
    "        \n",
    "        with open('misc_tf_idf.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open('misc_total_vocab.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open('misc_DF.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open('misc_origreg.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "        \n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        k = 5\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "        \n",
    "        sentences = csent\n",
    "        sentences.extend(isent)\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "        \n",
    "    \n",
    "    #print('Finding synonyms and hypernyms......')\n",
    "    #synonyms, query, qvocab = finalsynonyms(synwords, modelvocab, m, vocab, query, vdef, vocabdef, mainvocab)\n",
    "    \n",
    "    elif(category == 'legal case'):\n",
    "        rules = ltext\n",
    "        vocab = casevocab\n",
    "        N = 1496\n",
    "        \n",
    "        with open('case_tf_idf.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open('case_total_vocab.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open('case_DF.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open('case_origreg.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "        \n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        k = 5\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "        \n",
    "        sentences = lsent\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        lol = 0\n",
    "        #Amendment queries\n",
    "    \n",
    "    print('Extracting answer')\n",
    "    answer, ansreg = answermod(A, queryinput)\n",
    "    return answer, ansreg, Aold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QnAmodel(queryinput, docinput,k,category):\n",
    "    print('Preprocessing query......')\n",
    "    qvocab, query = queryvocab(queryinput)\n",
    "    if(category == 'regulations'):\n",
    "        rules, vocab, vdef, docinput = regextract(docinput, docs, docregs, mainvocab, vocabdef, queryinput, glossary)\n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('tf-idf calculation in progress.....')\n",
    "        DF, N, tf_idf, total_vocab, total_vocab_size, wordregs, regstring, origreg = tfidfreg(docregs,docinput,category)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "    elif(category == 'misc'):\n",
    "        \n",
    "        rules = ctext\n",
    "        rules.extend(itext)\n",
    "        \n",
    "        #for lol in docregs:\n",
    "        #    rules.extend(lol)\n",
    "        vocab = miscvocab\n",
    "        #vocab.extend(mainvocab)\n",
    "        N = 447\n",
    "        #N = 6379\n",
    "        \n",
    "        with open('misc_tf_idf.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open('misc_total_vocab.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open('misc_DF.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open('misc_origreg.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "        \n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        k = 5\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "        \n",
    "        sentences = csent\n",
    "        sentences.extend(isent)\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "        \n",
    "    \n",
    "    #print('Finding synonyms and hypernyms......')\n",
    "    #synonyms, query, qvocab = finalsynonyms(synwords, modelvocab, m, vocab, query, vdef, vocabdef, mainvocab)\n",
    "    \n",
    "    elif(category == 'legal case'):\n",
    "        rules = ltext\n",
    "        vocab = casevocab\n",
    "        N = 1496\n",
    "        \n",
    "        with open('case_tf_idf.pkl', 'rb') as f:\n",
    "            tf_idf = pickle.load(f)\n",
    "        with open('case_total_vocab.pkl', 'rb') as f:\n",
    "            total_vocab = pickle.load(f)\n",
    "        with open('case_DF.json') as f:\n",
    "            DF = json.load(f)\n",
    "        with open('case_origreg.pkl', 'rb') as f:\n",
    "            origreg = pickle.load(f)\n",
    "        \n",
    "        query, qvocab, synwords, importantwords, expansionwords = querypreprocess(query, qvocab, definitions, finaltopics)\n",
    "        print('Extracting relevant answer regulations.......')\n",
    "        k = 5\n",
    "        A = vectormodel(query, k, total_vocab, tf_idf, N, qvocab, DF, origreg, importantwords)\n",
    "        Aold = A\n",
    "        \n",
    "        sentences = lsent\n",
    "        Anew = []\n",
    "        for ans in A:\n",
    "            ind = rules.index(ans[1])\n",
    "            for sent in sentences[ind]:\n",
    "                Anew.append([ans[0],sent])\n",
    "        A = Anew\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        lol = 0\n",
    "        #Amendment queries\n",
    "    \n",
    "    print('Extracting answer')\n",
    "    #answer, ansreg = answermod(A, queryinput)\n",
    "    return Aold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing query......\n",
      "tf-idf calculation in progress.....\n",
      "Extracting relevant answer regulations.......\n",
      "Extracting answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepti-saravanan/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "queryinput = \"How does a company offer initial public offering\"\n",
    "docinput = 'All'\n",
    "category = 'regulations'\n",
    "A = QnAmodel(queryinput,docinput,10, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = []\n",
    "for i in A:\n",
    "    Ans.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries.append(queryinput)\n",
    "version1.append(Ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing query......\n",
      "tf-idf calculation in progress.....\n",
      "Extracting relevant answer regulations.......\n",
      "Extracting answer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deepti-saravanan/.local/lib/python3.7/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "A = QnAmodel(queryinput,docinput,50, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans = []\n",
    "for i in A:\n",
    "    Ans.append(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = Ans\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "query = queryinput\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "Ans1 = bm25.get_top_n(tokenized_query, corpus, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "version2.append(Ans1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_comp = []\n",
    "t=0\n",
    "while(t<len(test_queries)):\n",
    "    answers_comp.append([test_queries[t], version1[t], version2[t]])\n",
    "    t+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('answers_comparison.pkl','wb') as f:\n",
    "    pickle.dump(answers_comp,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept paper + informal guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to calculate the 5% creeping acquisition limit\n",
      "\n",
      "\n",
      "only gross acquisitions shall be taken into account\n",
      "\n",
      "\n",
      "c. For calculating the 5% creeping acquisition limit, as specified under regulation 3(2) of the Takeover Regulations, only gross acquisitions shall be taken into account. Any intermittent fall in shareholding owing to disposal of shares by the acquirer or dilution of shareholding on account of fresh issue of share capital shall be ignored.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the lock-in period duration in case of warrants\n",
      "\n",
      "\n",
      "6 (Six) Month\n",
      "\n",
      "\n",
      "3.1\tThe Lock-in period of 6 (Six) Month under Regulation 78(6) of ICDR Regulations, is to be determined on the \"date of trading approval\" under such provision. The warrants allotted to the warrant holder are not listed and hence concept of 'Trading Approval\" shall not be applicable to such warrants, until the same are converted in to equity shares. Accordingly, the date relevant for calculation of lock-in period for pre preferential holding, under Regulation 78(6) of the ICDR\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regulations + CP + IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to calculate the 5% creeping acquisition limit\n",
      "\n",
      "\n",
      "only gross acquisitions shall be taken into account\n",
      "\n",
      "\n",
      "c. For calculating the 5% creeping acquisition limit, as specified under regulation 3(2) of the Takeover Regulations, only gross acquisitions shall be taken into account. Any intermittent fall in shareholding owing to disposal of shares by the acquirer or dilution of shareholding on account of fresh issue of share capital shall be ignored.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the lock-in period duration in case of warrants\n",
      "\n",
      "\n",
      "6 (Six) Month\n",
      "\n",
      "\n",
      "3.1\tThe Lock-in period of 6 (Six) Month under Regulation 78(6) of ICDR Regulations, is to be determined on the \"date of trading approval\" under such provision. The warrants allotted to the warrant holder are not listed and hence concept of 'Trading Approval\" shall not be applicable to such warrants, until the same are converted in to equity shares. Accordingly, the date relevant for calculation of lock-in period for pre preferential holding, under Regulation 78(6) of the ICDR\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much amount will be provided as reward for informants in insider trading cases\n",
      "\n",
      "\n",
      "10 lakh\n",
      "\n",
      "\n",
      "Since the trading lot has been mandated as 10 lakh, participation in this platform is restricted to informed investors.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only regulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much amount will be provided as reward for informants in insider trading cases\n",
      "\n",
      "\n",
      "ten percent of the monetary sanctions collected or recovered and shall not exceed Rupees One crore\n",
      "\n",
      "\n",
      "7D. (1) Upon collection or substantial recovery of the monetary sanctions amounting to at least twice the Reward, the Board may at its sole discretion, declare an Informant eligible for Reward and intimate the Informant or his or her legal representative to file an application in the format provided in Schedule-E for claiming such Reward: Provided that the amount of Reward shall be ten percent of the monetary sanctions collected or recovered  and  shall  not  exceed  Rupees  One  crore  or  such  higher  amount  as  the  Board  may specify from time to time: Provided  further  that  the  Board  may  if  deemed  fit,  out  of  the  total  Reward  payable,  grant  an interim reward not exceeding Rupees Ten lacs or such higher amount as the Board may specify from  time  to  time,  on  the  issue  of  final  order  by  the  Board  against  the  person  directed  to disgorge.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legal cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the penalty amount for fraudulent and unfair trade activities\n",
      "\n",
      "\n",
      "twenty-five crore rupees\n",
      "\n",
      "\n",
      "6. In this regard the reliance on Hon’ble SAT order in the matter of Shailesh S. Jhaveri v. SEBI dated October, 4, 2012, wherein Hon’ble SAT examined the meaning of “disgorgement” is No applicable to the facts of this case, as the said order of Hon’ble SAT order is No in respect of liability of legal representatives for the disgorgement amount. 153. As observed earlier, Section 28B(2) of SEBI Act, 1992 enables SEBI to initiate action for disgorgement against the legal representatives after the death of the deceased. As per the aforesaid legal provisions, in conjunction with general law, and case laws cited above, I am Order in the matter of Kirloskar Brothers Limited Page 103 of 113 of the view that proceeding for disgorgement for unlawful gains loss avoided by Late Noticee No 6 survives his death. Therefore, proceeding for disgorgement can be initiated against LRs of Noticee No 6 and they are liable to pay the disgorgement amount, if any, under Sections 11(1) and 11B (1) of SEBI Act, 1992. ISSUE No.5: If issue No 1, 2, 3 & 4 is determined in affirmative in full or in part, then whether any directions including disgorgement under Sections 11(1), 11(4), 11B(1) of SEBI Act, 1992 and or monetary penalty under Sections 15G, 15HA and 15HB of SEBI Act, 1992 should be issued imposed against the respective Noticees LRs of Noticees for their respective violations? 154. I note that Noticee No 1 to 6, by trading in the shares of KBL, while in possession of UPSI- 1, had made notional unlawful gains avoided loss. The calculation of notional unlawful gains earned loss avoided by the Noticee No 1 to 6 is detailed at Paragraph 112 above. The amount of notional unlawful gains made loss avoided by Noticee No 1 to 6 is liable to be disgorged and the same is as under: Table No 18 Noticee No Noticee Name Unlawful ill-gotten Gain (Rs.) 1 Alpana Rahul Kirloskar 3,02,23,450 2 Arti Atul Kirloskar 3,02,23,450 3 Jyotsna Gautam Kulkarni 3,02,23,450 4 Rahul Chandrakant Kirloskar 2,51,54,950 5 Atul Chandrakant Kirloskar 2,51,54,950 6 Gautam Achyut Kulkarni – Since deceased * 2,51,54,950 Total 16,61,35,200 *Gautam Achyut Kulkarni had passed away on September 20, 2017, the legal representative of Late Gautam Achyut Kulkarni are (i) Jyotsna Gautam Kulkarni, (ii) Nihal Gautam Kulkarni and (iii) Ambar Gautam Kulkarni. 155. Further, I note that the Hon’ble Supreme Court of India in the matter of The Chairman, Sebi vs Shriram Mutual Fund & Anr decided on 23 May, 2006 held that “In our considered opinion, penalty is attracted as soon as the contravention of the statutory obligation as contemplated by the Act and the Regulation is established and hence the intention of the parties committing such violation becomes wholly irrelevant. A breach of civil obligation which attracts penalty in the nature of fine under the provisions of the Act and the Order in the matter of Kirloskar Brothers Limited Page 104 of 113 Regulations would immediately attract the levy of penalty irrespective of the fact whether contravention must made by the defaulter with guilty intention or not.....Hence once the contravention is established then the penalty is to follow”. 156. The provisions of Sections 15HA and 15HB of SEBI Act, 1992 as applicable at the time of transaction dated October 06, 2010 are as under: Penalty for fraudulent and unfair trade practices. 15HA. If any person indulges in fraudulent and unfair trade practices relating to securities, he shall be liable to a penalty of twenty-five crore rupees or three times the amount of profits made out of such failure, whichever is higher.” Penalty for contravention where no separate penalty has been provided. 15HB. Whoever fails to comply with any provision of this Act, the rules or the regulations made or directions issued by the Board thereunder for which no separate penalty has been provided, shall be liable to a penalty which may extend to one crore rupees. 157. Further, it is noted that Noticee No 6 died on September 20, 2017 and imposition of penalty is a personal action which terminates with the death of person. Hence, I am of the view that no penalty can be imposed on Noticee No\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the penalty amount if Noticee had acquired or disposed shares for which he was required to make disclosures\n",
      "\n",
      "\n",
      "ten lakh rupees\n",
      "\n",
      "\n",
      "7. (2) Continual Disclosures. (a). Every promoter, employee and director of every company shall disclose to the company the number of such securities acquired or disposed of within two trading days of such transaction if the value of the securities traded, whether in one transaction or a series of transactions over any calendar quarter, aggregates to a traded value in excess of ten lakh rupees or such other value as may be specified; Issue I: Whether Noticee has violated Regulation 7(2)(a) of PIT Regulations? Findings:\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the penalty amount if Noticee had acquired or disposed shares for which he was required to make disclosures under the provisions of regulations 7(1) & (2) of the SAST Regulations\n",
      "\n",
      "\n",
      "0.17%\n",
      "\n",
      "\n",
      "81. It was also alleged that duruing the same period, the Noticee No 2 had acquired 5.34% shares of the Velan Hotels and his holdings crossed 5% on November 26, 2009. As per regulation 13(1) of PIT Regulations and regulation 7(1) of SAST Regulations, the Noticee No 2 was required to make disclosure to the Company Velan Hotels and to the Stock Exchange where shares of the target company are listed, which he allegedly failed to make such disclosures. Further, during the period December 2009 to March 2010, the shareholding of Noticee No 2 had come down from 5.34% to 0.17% (i.e. change was more than 2% on March 17, 2010) and as per regulation 13 (3) of PIT Regulations, Noticee No 2 was required to make disclosures to the Company Velan Hotels which he had allegedly failed to disclose the same.\n"
     ]
    }
   ],
   "source": [
    "print(queryinput)\n",
    "print('\\n')\n",
    "print(answer)\n",
    "print('\\n')\n",
    "print(ansreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KG info for legal team validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/deepti-saravanan/Desktop/kg_connections/conceptpaper_graph.pkl','rb') as f:\n",
    "    cp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/deepti-saravanan/Desktop/kg_connections/ig.pickle','rb') as f:\n",
    "    ig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/deepti-saravanan/Desktop/kg_connections/lc.pickle','rb') as f:\n",
    "    lc = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "lc_df = pd.DataFrame(lc, columns = ['Regulatory Document','Regulation number','file_name','Topic represented by edges in graph'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_df.to_csv('legal_cases_graphinfo.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compbio",
   "language": "python",
   "name": "compbio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
